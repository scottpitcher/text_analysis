{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"data/customer_review_text.csv\")\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is extremely messy. While the data is complete, all columns [Text, Sentiment, Source, Date Time, User ID, Location, and Conf. score] are all within the same column. We will use regexp to separate them into different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(reviews.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, rename the column for ease of use later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = reviews.columns[0]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({\n",
    "   \"text\": reviews[col].str.extract(r'\"([^\"]+)\"', expand=False),\n",
    "   \"sentiment\": reviews[col].str.extract(r'(Positive|Negative)', expand=False),\n",
    "   \"source\": reviews[col].str.extract(r'\\S+, \\S+, (\\S+),', expand=False),\n",
    "   \"date\": reviews[col].str.extract(r'(\\d{4}-\\d{2}-\\d{2})', expand=False),\n",
    "   \"time\": reviews[col].str.extract(r'(\\d{2}:\\d{2}:\\d{2})', expand=False),\n",
    "   \"user\": reviews[col].str.extract(r'\\d{2}:\\d{2}:\\d{2}, (\\S+)', expand=False),\n",
    "   \"city\": reviews[col].str.extract(r'\\d{2}:\\d{2}:\\d{2}, \\S+, (.+?), 0\\.\\d{2}$', expand=False),\n",
    "   \"confidence\": reviews[col].str.extract(r'(0\\.\\d{2}$)', expand=False)\n",
    "   })\n",
    "\n",
    "df_clean[\"dateTime\"] = df_clean[\"date\"]+\" \"+df_clean[\"time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = df_clean.copy()\n",
    "reviews_df[\"sentiment\"] = np.where(reviews_df[\"sentiment\"] == \"Positive\", 1, np.where(reviews_df[\"sentiment\"] == \"Negative\", 0, np.nan))\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing datatypes\n",
    "from datetime import datetime\n",
    "categories = {\"text\":\"string\",\n",
    "              \"sentiment\":\"category\",\n",
    "              \"source\":\"category\",\n",
    "              \"date\": \"datetime64[ns]\",\n",
    "              \"time\":\"string\",\n",
    "              \"dateTime\":\"datetime64[ns]\",\n",
    "              \"user\":\"category\",\n",
    "              \"city\":\"category\",\n",
    "              \"confidence\":\"float\"}\n",
    "\n",
    "\n",
    "reviews_clean = reviews_df.astype(categories)\n",
    "reviews_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno \n",
    "\n",
    "print(msno.matrix(reviews_clean))\n",
    "print(reviews_clean.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the amount of missing per column, their locations, and specifically the amount in source, we'll use .dropna() subsetted for all other cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'reviews_clean' is your DataFrame\n",
    "reviews_clean.dropna(subset=reviews_clean.columns.difference(['source']), inplace= True)\n",
    "print(reviews_clean.isna().sum(),\n",
    "reviews_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews_clean[[\"text\",\"sentiment\"]].values.tolist()\n",
    "reviews[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling using CNN and a fully connected linear layer. Starting with embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "class CustomerReview(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence,label in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "# Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "    \n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = CustomerReview(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CNN Network for the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dict = dict(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(reviews_dict.keys()), list(reviews_dict.values()), test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = zip(X_train, y_train)\n",
    "test_data = zip(X_test, y_test)\n",
    "\n",
    "train_dataloader, train_vectorizer = text_processing_pipeline(train_data)\n",
    "test_dataloader, test_vectorizer = text_processing_pipeline(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        # Initialize the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(1, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, text):\n",
    "        text = text.unsqueeze(1)\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.fc(conved)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data.uniform_(-0.1, 0.1)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = TextClassificationCNN(vocab_size=len(train_vectorizer.get_feature_names_out()),embed_dim=50)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 20\n",
    "def training_loop(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            labels = torch.clamp(labels, 0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "accuracy_dict={}\n",
    "def testing_reviews():\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    i =0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels.squeeze()).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                accuracy = correct_predictions / total_samples\n",
    "                i+=1\n",
    "                if i == 11:\n",
    "                    break\n",
    "                return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc={}\n",
    "for i in np.arange(10,21,1):\n",
    "    training_loop(i)\n",
    "    accuracy = testing_reviews()\n",
    "    test_acc[i]=accuracy\n",
    "\n",
    "train_acc={}\n",
    "for i in np.arange(10,21,1):\n",
    "    training_loop(i)\n",
    "    accuracy = testing_reviews()\n",
    "    train_acc[i]=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_acc.keys(), test_acc.values(), marker='o', markerfacecolor='darkgreen', label='Test Accuracy')\n",
    "plt.plot(train_acc.keys(), train_acc.values(), marker='v', c='red', markerfacecolor='black', label='Train Accuracy')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Epoch vs. Accuracy for Model\")\n",
    "plt.legend()  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = max(test_acc, key=lambda k: accuracy_dict[k])\n",
    "max_accuracy = test_acc[max_epoch]\n",
    "\n",
    "print(f\"Maximum accuracy ({max_accuracy * 100:.2f}%) achieved at epoch {max_epoch + 1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
